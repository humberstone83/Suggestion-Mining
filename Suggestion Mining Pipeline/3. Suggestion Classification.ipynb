{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Suggestion Classification\n",
    "- MSc Computer Science - Final Major Project\n",
    "\n",
    "Instructions: Run all cells, then navigate to the bottom to view the results from the suggestion mining experiments conducted for my suggestion mining project.\n",
    "\n",
    "Estimated run time: 45mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',40)\n",
    "pd.set_option('display.max_rows',833)\n",
    "\n",
    "import numpy as np\n",
    "import os, gc, time, warnings\n",
    "\n",
    "from scipy import sparse\n",
    "import scipy.stats as ss\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import gensim\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Concatenate, BatchNormalization, Reshape\n",
    "from keras.layers import GRU, LSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, TimeDistributed\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Methods\n",
    "- Importing the data for training using pickle and storing them in pandas dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training data\n",
    "def import_train():\n",
    "    train = pickle.load( open( \"train_clean.pkl\", \"rb\" ) )\n",
    "    return train\n",
    "\n",
    "# Import training data (Lemmatized, and stop words removed)\n",
    "def import_train_swl():\n",
    "    train = pickle.load( open( \"train_clean2.pkl\", \"rb\" ) )\n",
    "    return train\n",
    "\n",
    "# Import training data (Augmented data)\n",
    "def import_train_aug(): \n",
    "    train = import_train()\n",
    "    train_aug1 = pickle.load( open( \"train_aug1.pkl\", \"rb\"))\n",
    "    train_stack = pd.concat([train, train_aug1], axis=0)\n",
    "    return train_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset A\n",
    "def import_dataset_A():\n",
    "    train = import_train()\n",
    "    val = pickle.load( open( \"test_clean.pkl\", \"rb\" ) )\n",
    "    test = pickle.load( open( \"eval_clean.pkl\", \"rb\" ) )\n",
    "    corpus = pickle.load( open( \"clean_corpus.pkl\", \"rb\"))\n",
    "    return train, val, test, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset A (Lemmatized, and stop words removed)\n",
    "def import_dataset_A_swl():\n",
    "    train = import_train_swl()\n",
    "    val = pickle.load( open( \"test_clean2.pkl\", \"rb\" ) )\n",
    "    test = pickle.load( open( \"eval_clean2.pkl\", \"rb\" ) )\n",
    "    corpus = pickle.load( open( \"clean_corpus.pkl\", \"rb\"))\n",
    "    return train, val, test, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset A (Augmented training data)\n",
    "def import_dataset_A_aug():\n",
    "    train = import_train_aug()\n",
    "    val = pickle.load( open( \"test_clean.pkl\", \"rb\" ) )\n",
    "    test = pickle.load( open( \"eval_clean.pkl\", \"rb\" ) )\n",
    "    corpus = pickle.load( open( \"clean_corpus.pkl\", \"rb\"))\n",
    "    return train, val, test, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset B\n",
    "def import_dataset_B():\n",
    "    train = import_train()\n",
    "    val = pickle.load( open( \"hotel_test_clean.pkl\", \"rb\" ) )\n",
    "    test = pickle.load( open( \"hotel_eval_clean.pkl\", \"rb\" ) )\n",
    "    corpus = pickle.load( open( \"clean_hotel_corpus.pkl\", \"rb\"))\n",
    "    return train, val, test, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset B (Lemmatized, and stop words removed)\n",
    "def import_dataset_B_swl():\n",
    "    train = import_train_swl()\n",
    "    val = pickle.load( open( \"hotel_test_clean.pkl\", \"rb\" ) )\n",
    "    test = pickle.load( open( \"hotel_eval_clean.pkl\", \"rb\" ) )\n",
    "    corpus = pickle.load( open( \"clean_hotel_corpus.pkl\", \"rb\"))\n",
    "    return train, val, test, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset B (Augmented training data)\n",
    "def import_dataset_B_aug():\n",
    "    train = import_train_aug()\n",
    "    val = pickle.load( open( \"hotel_test_clean.pkl\", \"rb\" ) )\n",
    "    test = pickle.load( open( \"hotel_eval_clean.pkl\", \"rb\" ) )\n",
    "    corpus = pickle.load( open( \"clean_hotel_corpus.pkl\", \"rb\"))\n",
    "    return train, val, test, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method for evaluating predictions and returning specific evaluation metrics\n",
    "def print_results(name, test_predictions, test_y):\n",
    "    print()\n",
    "    print('Confusion matrix:')\n",
    "    matrix = confusion_matrix(test_y, test_predictions)\n",
    "    print(matrix)\n",
    "\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    precision = precision_score(test_y, test_predictions)\n",
    "    recall = recall_score(test_y, test_predictions)\n",
    "    f1 = f1_score(test_y, test_predictions)\n",
    " \n",
    "    results = pd.DataFrame([[name, accuracy, precision, recall, f1]], columns=('Classifier', 'Accuracy', 'Precision', 'Recall', 'F-Score'))\n",
    "    print(results)\n",
    "    \n",
    "    return results, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method for plotting the accuracy and the loss of each model\n",
    "#  This method was taken and adapted from:\n",
    "# (https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/)\n",
    "def plot_history(trainingMem):\n",
    "    acc = trainingMem.history['accuracy']\n",
    "    eval_acc = trainingMem.history['val_accuracy']\n",
    "    loss = trainingMem.history['loss']\n",
    "    eval_loss = trainingMem.history['val_loss']\n",
    "    x = range(1, len(acc) + 1) \n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(x,acc,'b',label='Training accuracy')\n",
    "    plt.plot(x,eval_acc,'r',label='Evaluation accuracy')\n",
    "    plt.title('Training and evaluation accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(x,loss,'b',label='Training loss')\n",
    "    plt.plot(x,eval_loss,'r',label='Evaluation loss')\n",
    "    plt.title('Training and evaluation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "- Classification models require numerical inputs in the form of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method for spliting a pandas dataframe into an x and y set of sentences and labels\n",
    "def x_y_split(train, val, test):\n",
    "    \n",
    "    train_x = train['review'].values\n",
    "    train_y = train['label'].values\n",
    "\n",
    "    val_x = val['review'].values\n",
    "    val_y = val['label'].values\n",
    "\n",
    "    test_x = test['review'].values\n",
    "    test_y = test['label'].values \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "\n",
    "def y_split(train, val, test):\n",
    "        \n",
    "    train_x = train['review'].values\n",
    "    train_y = train['label'].values\n",
    "\n",
    "    val_x = val['review'].values\n",
    "    val_y = val['label'].values\n",
    "\n",
    "    test_x = test['review'].values\n",
    "    test_y = test['label'].values \n",
    "    return train_y, val_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method for calling and returning 3 seperate vectorizers with different ngram ranges \n",
    "def fetch_ngram_vectorizer(corpus):\n",
    "    UGVectorizer = CountVectorizer(min_df=0,lowercase=False, ngram_range=(1,1))\n",
    "    BGVectorizer = CountVectorizer(min_df=0,lowercase=False, ngram_range=(2,3))\n",
    "    CGVectorizer = CountVectorizer(min_df=0,lowercase=False, ngram_range=(1,5))\n",
    "    UGVectorizer.fit(corpus)\n",
    "    BGVectorizer.fit(corpus)\n",
    "    CGVectorizer.fit(corpus)\n",
    "    return UGVectorizer, BGVectorizer, CGVectorizer\n",
    "\n",
    "# A method for calling and returning 3 seperate vectorizers with different ngram ranges, and TF-IDF weighting\n",
    "def fetch_ngram_vectorizer_tfidf(corpus):\n",
    "    UG_tfidf = TfidfVectorizer(min_df=0,lowercase=False, ngram_range=(1,1))\n",
    "    BG_tfidf = TfidfVectorizer(min_df=0,lowercase=False, ngram_range=(2,3))\n",
    "    CG_tfidf = TfidfVectorizer(min_df=0,lowercase=False, ngram_range=(1,5))\n",
    "    UG_tfidf.fit(corpus)\n",
    "    BG_tfidf.fit(corpus)\n",
    "    CG_tfidf.fit(corpus)\n",
    "    return UG_tfidf, BG_tfidf, CG_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method for vectorizing my data with different ngram ranges, returns the different vectors stacked together\n",
    "def transform_ngram_vec(train,val,test,corpus):\n",
    "    UniVec, BiVec, CharVec = fetch_ngram_vectorizer(corpus)\n",
    "    \n",
    "    U_train = UniVec.transform(train)\n",
    "    U_val = UniVec.transform(val)\n",
    "    U_test = UniVec.transform(test)\n",
    "    \n",
    "    B_train = BiVec.transform(train)\n",
    "    B_val = BiVec.transform(val)\n",
    "    B_test = BiVec.transform(test)\n",
    "    \n",
    "    C_train = CharVec.transform(train)\n",
    "    C_val = CharVec.transform(val)\n",
    "    C_test = CharVec.transform(test)\n",
    "    \n",
    "    train_X = hstack((U_train, B_train, C_train)) \n",
    "    val_X = hstack((U_val, B_val, C_val)) \n",
    "    test_X = hstack((U_test, B_test, C_test))\n",
    "    \n",
    "    return train_X, val_X, test_X\n",
    "\n",
    "# A method for vectorizing my data with different ngram ranges, and tf-idf weighting.\n",
    "# Returns the different vectors stacked together\n",
    "def transform_ngram_vec_tfidf(train,val,test,corpus):\n",
    "    UniVec, BiVec, CharVec = fetch_ngram_vectorizer_tfidf(corpus)\n",
    "    \n",
    "    U_train = UniVec.transform(train)\n",
    "    U_val = UniVec.transform(val)\n",
    "    U_test = UniVec.transform(test)\n",
    "    \n",
    "    B_train = BiVec.transform(train)\n",
    "    B_val = BiVec.transform(val)\n",
    "    B_test = BiVec.transform(test)\n",
    "    \n",
    "    C_train = CharVec.transform(train)\n",
    "    C_val = CharVec.transform(val)\n",
    "    C_test = CharVec.transform(test)\n",
    "    \n",
    "    train_X = hstack((U_train, B_train, C_train)) \n",
    "    val_X = hstack((U_val, B_val, C_val)) \n",
    "    test_X = hstack((U_test, B_test, C_test))\n",
    "    \n",
    "    return train_X, val_X, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns the logistic regression classifier\n",
    "def fetch_logr(train_X, train_y):\n",
    "    logReg = LogisticRegression(solver='lbfgs',max_iter=10000)\n",
    "    logReg.fit(train_X, train_y)\n",
    "    return logReg\n",
    "\n",
    "# This method applys logistic regression to the data, it returns predicted classes\n",
    "def logr_predict(train_X, train_y, val_X, val_y, test_X, test_y):\n",
    "    logReg = fetch_logr(train_X, train_y)\n",
    "    lr_Accuracy = logReg.score(train_X, train_y)\n",
    "    lr_Accuracy_eval = logReg.score(val_X, val_y)\n",
    "\n",
    "    test_predictions = logReg.predict(test_X)\n",
    "\n",
    "    print(\"Logistic Regression...\")\n",
    "    print(\"Training-Accuracy:\" + str(lr_Accuracy))\n",
    "    print(\"Validation-Accuracy:\" + str(lr_Accuracy_eval))\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method evaluates the logistic regression classifier,\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.\n",
    "def logistic_regression(train, val, test, corpus):\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = x_y_split(train, val, test)\n",
    "    train_X, val_X, test_X = transform_ngram_vec(train_x,val_x,test_x,corpus)\n",
    "    logr_predictions = logr_predict(train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "    logr_results, logr_matrix = print_results(\"Logistic Regression\", logr_predictions, test_y)\n",
    "    return logr_results, logr_matrix \n",
    "\n",
    "# This method evaluates the logistic regression classifier with tf-idf weighting,\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.\n",
    "def logistic_regression_tfidf(train, val, test, corpus):\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = x_y_split(train, val, test)\n",
    "    train_X, val_X, test_X = transform_ngram_vec_tfidf(train_x,val_x,test_x,corpus)\n",
    "    logr_predictions = logr_predict(train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "    logr_results, logr_matrix = print_results(\"Logistic Regression (TF-IDF)\", logr_predictions, test_y)\n",
    "    return logr_results, logr_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method performs truncated singular value decomposition on the input\n",
    "# This particlar method was taken and adapted from:\n",
    "# (https://www.kaggle.com/sanketrai/suggestion-mining)\n",
    "def truncated_svd(x_train, x_val, x_test):\n",
    "    svd = TruncatedSVD(n_components = 15)\n",
    "    svd.fit(vstack((x_train, x_val, x_test)).tocsr())\n",
    "    x_train_svd = svd.transform(x_train)\n",
    "    x_val_svd = svd.transform(x_val)\n",
    "    x_test_svd = svd.transform(x_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate((x_train_svd, x_val_svd, x_test_svd)))\n",
    "    x_train_svd = scaler.transform(x_train_svd)\n",
    "    x_val_svd = scaler.transform(x_val_svd)\n",
    "    x_test_svd = scaler.transform(x_test_svd)\n",
    "    return x_train_svd, x_val_svd, x_test_svd\n",
    "\n",
    "# This method returns the suport vector machines\n",
    "def fetch_SVM(x_train_svd, train_y):\n",
    "    SVM = SVC(C = 0.1, probability = True)\n",
    "    SVM.fit(x_train_svd, train_y)\n",
    "    return SVM\n",
    "\n",
    "# This method performs predictions using the support vector machines\n",
    "def SVM_predict(train_X, train_y, val_X, val_y, test_X, test_y):\n",
    "    x_train_svd, x_val_svd, x_test_svd = truncated_svd(train_X, val_X, test_X)\n",
    "    SVM = fetch_SVM(x_train_svd, train_y)\n",
    "    \n",
    "    svm_Accuracy = SVM.score(x_train_svd, train_y)\n",
    "    svm_Accuracy_val = SVM.score(x_val_svd, val_y)\n",
    "    svm_test_pred = SVM.predict(x_test_svd)\n",
    "\n",
    "    print(\"Support Vector Machine...\")\n",
    "    print(\"Training-Accuracy:\" + str(svm_Accuracy))\n",
    "    print(\"Validation-Accuracy:\" + str(svm_Accuracy_val))\n",
    "    return svm_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method evaluates the support vector machines classifier,\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.\n",
    "def support_vector_machine(train, val, test, corpus):\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = x_y_split(train, val, test)\n",
    "    train_X, val_X, test_X = transform_ngram_vec(train_x,val_x,test_x,corpus)\n",
    "    predictions = SVM_predict(train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "    SVM_results, SVM_matrix = print_results(\"Support Vector Machine\", predictions, test_y)\n",
    "    return SVM_results, SVM_matrix \n",
    "\n",
    "# This method evaluates the support vector machines classifier with tf-idf,\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.\n",
    "def support_vector_machine_tfidf(train, val, test, corpus):\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = x_y_split(train, val, test)\n",
    "    train_X, val_X, test_X = transform_ngram_vec_tfidf(train_x,val_x,test_x,corpus)\n",
    "    predictions = SVM_predict(train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "    SVM_results, SVM_matrix = print_results(\"Support Vector Machine (TF-IDF)\", predictions, test_y)\n",
    "    return SVM_results, SVM_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return word_embeddings, also return vocab_length\n",
    "def train_word_embeddings(train, val, test, corpus, max_length):\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    x_train = train['review'].values \n",
    "    x_val = val['review'].values \n",
    "    x_test = test['review'].values \n",
    "    \n",
    "    x_train = tokenizer.texts_to_sequences(x_train)\n",
    "    x_val = tokenizer.texts_to_sequences(x_val)\n",
    "    x_test = tokenizer.texts_to_sequences(x_test)\n",
    "    \n",
    "    vocab_len = len(tokenizer.word_index) +1\n",
    "    \n",
    "    train_X = pad_sequences(x_train, padding='post', maxlen=max_length)\n",
    "    eval_X = pad_sequences(x_val, padding='post', maxlen=max_length)\n",
    "    test_X = pad_sequences(x_test,padding='post', maxlen=max_length)\n",
    "    \n",
    "    return vocab_len, train_X, eval_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return word_embeddings, also return vocab_length, max_length and labels\n",
    "def vectorize_embeddings(train, val, test, corpus):\n",
    "    max_length = 100\n",
    "\n",
    "    train_y = train['label'].values\n",
    "    val_y = val['label'].values\n",
    "    test_y = test['label'].values \n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    x_train = train['review'].values\n",
    "    x_val = val['review'].values\n",
    "    x_test = test['review'].values\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(x_train)\n",
    "    val_X = tokenizer.texts_to_sequences(x_val)\n",
    "    test_X = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "    vocab_len = len(tokenizer.word_index) +1\n",
    "    \n",
    "    train_X = pad_sequences(train_X, padding='post', maxlen=max_length)\n",
    "    val_X = pad_sequences(val_X, padding='post', maxlen=max_length)\n",
    "    test_X = pad_sequences(test_X,padding='post', maxlen=max_length)\n",
    "    return vocab_len, max_length, train_X, train_y, val_X, val_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return embedding matrix\n",
    "# This method was taken from:\n",
    "# (https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1 \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return glove embedding matrix\n",
    "# This method was taken from:\n",
    "# (https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n",
    "def create_glove_matrix(corpus):\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    embedding_dim = 300\n",
    "    embedding_matrix = create_embedding_matrix('GloVe/glove.42B.300d.txt', tokenizer.word_index, embedding_dim)\n",
    "    return embedding_dim, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and return part of speech tags, return word embeddings\n",
    "def pos_tokenize(train, val, test, corpus, max_length):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    pos_tags_train = train['review'].apply(lambda x : \" \".join(item[1] for item in pos_tag(word_tokenize(x)))).values\n",
    "    pos_tags_val = val['review'].apply(lambda x : \" \".join(item[1] for item in pos_tag(word_tokenize(x)))).values\n",
    "    pos_tags_test = test['review'].apply(lambda x : \" \".join(item[1] for item in pos_tag(word_tokenize(x)))).values\n",
    "  \n",
    "    train_POS = tokenizer.texts_to_sequences(pos_tags_train)\n",
    "    val_POS = tokenizer.texts_to_sequences(pos_tags_val)\n",
    "    test_POS = tokenizer.texts_to_sequences(pos_tags_test)\n",
    "    \n",
    "    train_POS = pad_sequences(train_POS, padding='post', maxlen=max_length)\n",
    "    val_POS = pad_sequences(val_POS, padding='post', maxlen=max_length)\n",
    "    test_POS = pad_sequences(test_POS,padding='post', maxlen=max_length)\n",
    "    \n",
    "    return train_POS, val_POS, test_POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "- Methods to present results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to fit on training data, as well as to make predictions for a test dataset\n",
    "def NN_predict(model, train_X, train_y, val_X, val_y, test_X, test_y):\n",
    "    trainingMem = model.fit(train_X, train_y, epochs=10, \n",
    "                            verbose=False, validation_data=(val_X, val_y), \n",
    "                            batch_size=128, callbacks=early_stopping())\n",
    "    \n",
    "#     Uncomment the following line for graphs plotting the accuracy and loss over each epoch\n",
    "#     plot_history(trainingMem)\n",
    "    \n",
    "    predictions = model.predict(test_X, batch_size = 128, verbose = 1)\n",
    "\n",
    "    classes = (predictions[:, 0] >= 0.5).astype(int)\n",
    "\n",
    "    loss, accuracy = model.evaluate(train_X, train_y, verbose = False)\n",
    "    print('training data...')\n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    print('Loss:' + str(loss))\n",
    "\n",
    "    loss, accuracy = model.evaluate(val_X, val_y, verbose = False)\n",
    "    print('validation data...')\n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    print('Loss:' + str(loss))\n",
    "    \n",
    "    return classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to fit on training data, as well as to make predictions for a test dataset\n",
    "def NN_predict2(train_X, train_y, val_X, val_y, test_X, test_y, model):\n",
    "    \n",
    "    trainingMem = model.fit(x = [train_X], y=train_y, epochs=5, \n",
    "                            verbose=False, validation_data=([val_X], val_y), \n",
    "                            batch_size=128, callbacks=early_stopping())\n",
    "\n",
    "#     Uncomment the following line for graphs plotting the accuracy and loss over each epoch\n",
    "#     plot_history(trainingMem)\n",
    "    \n",
    "    predictions = model.predict([test_X], batch_size = 128, verbose = 1)\n",
    "\n",
    "    classes = (predictions[:, 0] >= 0.5).astype(int)\n",
    "\n",
    "    loss, accuracy = model.evaluate([train_X], train_y, verbose = False)\n",
    "    print('training data...')\n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    print('Loss:' + str(loss))\n",
    "\n",
    "    loss, accuracy = model.evaluate([val_X], val_y, verbose = False)\n",
    "    print('validation data...')\n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    print('Loss:' + str(loss))\n",
    "    \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to fit on training data, as well as to make predictions for a test dataset with multiple inputs\n",
    "def NN_predict_pos(train_X, train_POS, train_y, val_X, val_POS, val_y, test_X, test_POS, test_y, model):\n",
    "    trainingMem = model.fit(x = [train_X, train_POS], y=train_y, epochs=5, \n",
    "                        verbose=False, validation_data=([val_X, val_POS], val_y), \n",
    "                        batch_size=128, callbacks=early_stopping())\n",
    "    \n",
    "#     Uncomment the following line for graphs plotting the accuracy and loss over each epoch\n",
    "#     plot_history(trainingMem)\n",
    "    \n",
    "    Com_POS_test_pred_p = model.predict([test_X, test_POS], batch_size = 128, verbose = 1)\n",
    "\n",
    "    Com_POS_test_pred_c = (Com_POS_test_pred_p[:, 0] >= 0.5).astype(int)\n",
    "\n",
    "    loss, accuracy = model.evaluate([train_X, train_POS], train_y, verbose = False)\n",
    "    print('training data...')\n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    print('Loss:' + str(loss))\n",
    "\n",
    "\n",
    "    loss, accuracy = model.evaluate([val_X, val_POS], val_y, verbose = False)\n",
    "    print('validation data...')\n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    print('Loss:' + str(loss)) \n",
    "    return Com_POS_test_pred_c \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback method early_stopping\n",
    "def early_stopping():\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=1, verbose=1)\n",
    "    callbacks_list = [early_stopping]\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Short Term Memory Model\n",
    "def LSTM_model(vocab_length,embedding_dim,max_length):\n",
    "    LSTM_model = Sequential()\n",
    "    LSTM_model.add(layers.Embedding(input_dim=vocab_length, \n",
    "                               output_dim=embedding_dim, \n",
    "                               input_length=max_length,\n",
    "                                 trainable=True))\n",
    "\n",
    "    LSTM_model.add(LSTM(units = 150, return_sequences = True))\n",
    "    LSTM_model.add(layers.MaxPooling1D())\n",
    "    LSTM_model.add(LSTM(units = 150, return_sequences = True))\n",
    "    LSTM_model.add(layers.GlobalMaxPooling1D())\n",
    "    LSTM_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    LSTM_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     LSTM_model.summary()\n",
    "    return LSTM_model\n",
    "\n",
    "# Long Short Term Memory Model - Glove\n",
    "def PTWE_LSTM_model(embedding_matrix, vocab_len, embedding_dim, max_length):\n",
    "    LSTM_model = Sequential()\n",
    "    LSTM_model.add(layers.Embedding(vocab_len, embedding_dim, \n",
    "                              weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "\n",
    "    LSTM_model.add(LSTM(units = 150, return_sequences = True))\n",
    "    LSTM_model.add(layers.MaxPooling1D())\n",
    "    LSTM_model.add(LSTM(units = 150, return_sequences = True))\n",
    "    LSTM_model.add(layers.GlobalMaxPooling1D())\n",
    "    LSTM_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    LSTM_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     LSTM_model.summary()\n",
    "    return LSTM_model\n",
    "\n",
    "# Long Short Term Memory Model - POS tags\n",
    "def LSTM_model_pos(vocab_len,embedding_dim,max_length):\n",
    "    inp = Input(shape=(100,))\n",
    "    pos = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=False)(inp)\n",
    "    pos_in = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=True)(pos)\n",
    "    x = Concatenate()([emb_word,pos_in])\n",
    "    \n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.LSTM(units = 50, return_sequences = True)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[inp,pos], outputs=out)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "# Long Short Term Memory Model - Glove & POS tags\n",
    "def PTWE_LSTM_model_pos(embedding_matrix, vocab_len, embedding_dim, max_length):\n",
    "    inp = Input(shape=(100,))\n",
    "    pos = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, \n",
    "                              weights=[embedding_matrix], input_length=max_length, trainable=False)(inp)\n",
    "    pos_in = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=True)(pos)\n",
    "    x = Concatenate()([emb_word,pos_in])\n",
    "    \n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.LSTM(units = 50, return_sequences = True)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[inp,pos], outputs=out)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method creates an embedding matrix and returns the LSTM POS tag model\n",
    "def fetch_glove_lstm_model_pos(corpus, max_length, vocab_len):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    model = HYB_model_glove_pos(embedding_matrix, max_length, embedding_dim, vocab_len)\n",
    "    return model\n",
    "\n",
    "# This method fetches the LSTM POS tag model\n",
    "def fetch_lstm_model_pos(embedding_dim, max_length, vocab_len):\n",
    "    model = HYB_model_pos(max_length, embedding_dim, vocab_len)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method evaluates the LSTM classifier,\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.\n",
    "def lstm(train, val, test, corpus):\n",
    "    embedding_dim = 50\n",
    "    max_length = 100\n",
    "    train_y, val_y, test_y = y_split(train, val, test)\n",
    "    vocab_len, train_X, val_X, test_X = train_word_embeddings(train, val, test, corpus, max_length)\n",
    "    model =  LSTM_model(vocab_len,embedding_dim,max_length)\n",
    "    predictions = NN_predict(model, train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "    return print_results(\"LSTM\", predictions, test_y)\n",
    "   \n",
    "# This method evaluates the LSTM classifier, using glove word embeddings\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.    \n",
    "def lstm_glove(train, val, test, corpus):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    max_length = 100\n",
    "    train_y, val_y, test_y = y_split(train, val, test)\n",
    "    vocab_len, train_X, val_X, test_X = train_word_embeddings(train, val, test, corpus, max_length)\n",
    "    model =  PTWE_LSTM_model(embedding_matrix, vocab_len, embedding_dim, max_length)\n",
    "    predictions = NN_predict(model, train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "    results, matrix = print_results(\"LSTM - Glove\", predictions, test_y)  \n",
    "    return results, matrix\n",
    "\n",
    "# This method evaluates the LSTM classifier, using part of speech tags\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.\n",
    "def lstm_pos(train, val, test, corpus):\n",
    "    embedding_dim = 50\n",
    "    max_length = 100\n",
    "    train_y, val_y, test_y = y_split(train, val, test)\n",
    "    vocab_len, train_X, val_X, test_X = train_word_embeddings(train, val, test, corpus, max_length)\n",
    "    train_POS, val_POS, test_POS = pos_tokenize(train, val, test, corpus, max_length)\n",
    "    model =  fetch_lstm_model_pos(embedding_dim,max_length, vocab_len)\n",
    "    predictions = NN_predict_pos(train_X, train_POS, train_y, val_X, val_POS, val_y, test_X, test_POS, test_y, model)\n",
    "    return print_results(\"LSTM - POS\", predictions, test_y)\n",
    "   \n",
    "# This method evaluates the LSTM classifier, using glove word embeddings and part of speech tags\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.    \n",
    "def lstm_glove_pos(train, val, test, corpus):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    max_length = 100\n",
    "    train_y, val_y, test_y = y_split(train, val, test)\n",
    "    vocab_len, train_X, val_X, test_X = train_word_embeddings(train, val, test, corpus, max_length)\n",
    "    train_POS, val_POS, test_POS = pos_tokenize(train, val, test, corpus, max_length)\n",
    "    model =  fetch_glove_lstm_model_pos(corpus, max_length, vocab_len)\n",
    "    predictions = NN_predict_pos(train_X, train_POS, train_y, val_X, val_POS, val_y, test_X, test_POS, test_y, model)\n",
    "    results, matrix = print_results(\"LSTM Glove & POS\", predictions, test_y)  \n",
    "    return results, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network Model\n",
    "def CNN_model(vocab_length,embedding_dim,max_length):\n",
    "    CNN_model = Sequential()\n",
    "    CNN_model.add(layers.Embedding(input_dim=vocab_length, \n",
    "                               output_dim=embedding_dim, \n",
    "                               input_length=max_length,\n",
    "                                 trainable=True))\n",
    "\n",
    "    CNN_model.add(layers.Conv1D(64, 7, activation='relu'))\n",
    "    CNN_model.add(layers.MaxPooling1D(2))\n",
    "    CNN_model.add(layers.Conv1D(64, 7, activation='relu'))\n",
    "    CNN_model.add(layers.GlobalMaxPooling1D())\n",
    "    CNN_model.add(layers.Dropout(0.5))\n",
    "    CNN_model.add(layers.Dense(32, activation='relu'))\n",
    "    CNN_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    CNN_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#     CNN_model.summary()\n",
    "    return CNN_model\n",
    "\n",
    "# Convolutional Neural Network Model - Glove\n",
    "def PTWE_CNN_model(embedding_matrix, vocab_len, embedding_dim, max_length):\n",
    "    CNN_model = Sequential()\n",
    "    CNN_model.add(layers.Embedding(vocab_len, embedding_dim, \n",
    "                              weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "\n",
    "    CNN_model.add(layers.Conv1D(64, 7, activation='relu'))\n",
    "    CNN_model.add(layers.MaxPooling1D(2))\n",
    "    CNN_model.add(layers.Conv1D(64, 7, activation='relu'))\n",
    "    CNN_model.add(layers.GlobalMaxPooling1D())\n",
    "    CNN_model.add(layers.Dropout(0.5))\n",
    "    CNN_model.add(layers.Dense(32, activation='relu'))\n",
    "    CNN_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    CNN_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#     CNN_model.summary()\n",
    "    return CNN_model\n",
    "\n",
    "# Convolutional Neural Network Model - POS tags\n",
    "def CNN_model_pos(vocab_length,embedding_dim,max_length):\n",
    "    inp = Input(shape=(100,))\n",
    "    pos = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=False)(inp)\n",
    "    pos_in = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=True)(pos)\n",
    "    x = Concatenate()([emb_word,pos_in])\n",
    "\n",
    "    x = layers.Conv1D(64, 7, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(64, 7, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[inp,pos], outputs=out)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "# Convolutional Neural Network Model - Glove & POS tags\n",
    "def PTWE_CNN_model_pos(embedding_matrix, vocab_len, embedding_dim, max_length):\n",
    "    inp = Input(shape=(100,))\n",
    "    pos = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, \n",
    "                              weights=[embedding_matrix], input_length=max_length, trainable=False)(inp)\n",
    "    pos_in = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=True)(pos)\n",
    "    x = Concatenate()([emb_word,pos_in])\n",
    "\n",
    "    x = layers.Conv1D(64, 7, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(64, 7, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[inp,pos], outputs=out)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch CNN model and create glove matrix - POS tags model\n",
    "def fetch_glove_CNN_model_pos(corpus, max_length, vocab_len):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    model = HYB_model_glove_pos(embedding_matrix, max_length, embedding_dim, vocab_len)\n",
    "    return model\n",
    "\n",
    "# fetch CNN model - POS tags model\n",
    "def fetch_CNN_model_pos(embedding_dim, max_length, vocab_len):\n",
    "    model = HYB_model_pos(max_length, embedding_dim, vocab_len)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method evaluates the CNN classifier,\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.\n",
    "def cnn(train, val, test, corpus):\n",
    "    embedding_dim = 50\n",
    "    max_length = 100\n",
    "    train_y, val_y, test_y = y_split(train, val, test)\n",
    "    vocab_len, train_X, val_X, test_X = train_word_embeddings(train, val, test, corpus, max_length)\n",
    "    model =  CNN_model(vocab_len,embedding_dim,max_length)\n",
    "    predictions = NN_predict(model, train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "    return print_results(\"CNN\", predictions, test_y)\n",
    "   \n",
    "# This method evaluates the CNN classifier, with Glove pretrained word embeddings\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.    \n",
    "def cnn_glove(train, val, test, corpus):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    max_length = 100\n",
    "    train_y, val_y, test_y = y_split(train, val, test)\n",
    "    vocab_len, train_X, val_X, test_X = train_word_embeddings(train, val, test, corpus, max_length)\n",
    "    model =  PTWE_CNN_model(embedding_matrix, vocab_len, embedding_dim, max_length)\n",
    "    predictions = NN_predict(model, train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "    results, matrix = print_results(\"CNN - Glove\", predictions, test_y)  \n",
    "    return results, matrix\n",
    "\n",
    "# This method evaluates the CNN classifier, with POS tags\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.\n",
    "def cnn_pos(train, val, test, corpus):\n",
    "    embedding_dim = 50\n",
    "    max_length = 100\n",
    "    train_y, val_y, test_y = y_split(train, val, test)\n",
    "    vocab_len, train_X, val_X, test_X = train_word_embeddings(train, val, test, corpus, max_length)\n",
    "    train_POS, val_POS, test_POS = pos_tokenize(train, val, test, corpus, max_length)\n",
    "    model =  fetch_CNN_model_pos(embedding_dim,max_length, vocab_len)\n",
    "    predictions = NN_predict_pos(train_X, train_POS, train_y, val_X, val_POS, val_y, test_X, test_POS, test_y, model)\n",
    "    return print_results(\"CNN - POS\", predictions, test_y)\n",
    "   \n",
    "# This method evaluates the CNN classifier, with Glove and POS tags\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data.    \n",
    "def cnn_glove_pos(train, val, test, corpus):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    max_length = 100\n",
    "    train_y, val_y, test_y = y_split(train, val, test)\n",
    "    vocab_len, train_X, val_X, test_X = train_word_embeddings(train, val, test, corpus, max_length)\n",
    "    train_POS, val_POS, test_POS = pos_tokenize(train, val, test, corpus, max_length)\n",
    "    model =  fetch_glove_CNN_model_pos(corpus, max_length, vocab_len)\n",
    "    predictions = NN_predict_pos(train_X, train_POS, train_y, val_X, val_POS, val_y, test_X, test_POS, test_y, model)\n",
    "    results, matrix = print_results(\"CNN - Glove & POS\", predictions, test_y)  \n",
    "    return results, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional - Long Short Term Memory model - Glove\n",
    "def HYB_model_glove(embedding_matrix, max_length, embedding_dim, vocab_len):\n",
    "    inp = Input(shape=(100,))\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, \n",
    "                              weights=[embedding_matrix], input_length=max_length, trainable=False)(inp)\n",
    "    x = emb_word\n",
    "\n",
    "    x = layers.Conv1D(120, 9, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(80, 9, activation='relu')(x)\n",
    "    \n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=out)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "# Convolutional - Long Short Term Memory model\n",
    "def HYB_model(max_length, embedding_dim, vocab_len):\n",
    "    inp = Input(shape=(100,))\n",
    "    emb_word = layers.Embedding(input_dim=vocab_len, \n",
    "                               output_dim=embedding_dim, \n",
    "                               input_length=max_length,\n",
    "                                 trainable=True)(inp)\n",
    "    x = emb_word\n",
    "\n",
    "    x = layers.Conv1D(120, 9, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(80, 9, activation='relu')(x)\n",
    "    \n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=out)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "# Convolutional - Long Short Term Memory model - Glove & POS tags\n",
    "def HYB_model_glove_pos(embedding_matrix, max_length, embedding_dim, vocab_len):\n",
    "    inp = Input(shape=(100,))\n",
    "    pos = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, \n",
    "                              weights=[embedding_matrix], input_length=max_length, trainable=False)(inp)\n",
    "    \n",
    "    pos_in = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=True)(pos)\n",
    "    x = Concatenate()([emb_word,pos_in])\n",
    "\n",
    "    x = layers.Conv1D(120, 9, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(80, 9, activation='relu')(x)\n",
    "    \n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[inp,pos], outputs=out)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "# Convolutional - Long Short Term Memory model - POS tags\n",
    "def HYB_model_pos(max_length, embedding_dim, vocab_len):\n",
    "    inp = Input(shape=(100,))\n",
    "    pos = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=False)(inp)\n",
    "    \n",
    "    pos_in = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=True)(pos)\n",
    "    x = Concatenate()([emb_word,pos_in])\n",
    "\n",
    "    x = layers.Conv1D(120, 9, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(80, 9, activation='relu')(x)\n",
    "    \n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.LSTM(units = 150, return_sequences = True)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[inp,pos], outputs=out)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the glove embedding matrix and returns the model\n",
    "def hyb_glove_model(corpus, max_length, vocab_len):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    model = HYB_model_glove(embedding_matrix, max_length, embedding_dim, vocab_len)\n",
    "    return model\n",
    "\n",
    "# fetches the model\n",
    "def hyb_model(embedding_dim, max_length, vocab_len):\n",
    "    model = HYB_model(max_length, embedding_dim, vocab_len)\n",
    "    return model\n",
    "\n",
    "# creates the glove embedding matrix and fetches the model\n",
    "def hyb_glove_model_pos(corpus, max_length, vocab_len):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    model = HYB_model_glove_pos(embedding_matrix, max_length, embedding_dim, vocab_len)\n",
    "    return model\n",
    "\n",
    "# fetches the model\n",
    "def hyb_model_pos(embedding_dim, max_length, vocab_len):\n",
    "    model = HYB_model_pos(max_length, embedding_dim, vocab_len)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This method evaluates the C-LSTM classifier,\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data. \n",
    "def clstm(train, val, test, corpus):\n",
    "    embedding_dim = 300\n",
    "    vocab_len, max_length, train_X, train_y, val_X, val_y, test_X, test_y = vectorize_embeddings(train, val, test, corpus)\n",
    "    model = hyb_model(embedding_dim, max_length, vocab_len)\n",
    "    predictions = NN_predict2(train_X, train_y, val_X, val_y, test_X, test_y, model)\n",
    "    results, matrix = print_results(\"C-LSTM\", predictions, test_y)  \n",
    "    return results, matrix\n",
    "\n",
    "#  This method evaluates the C-LSTM classifier, with Glove \n",
    "# It takes in a split dataset but returns evaluation metrics from the test data. \n",
    "def clstm_glove(train, val, test, corpus):\n",
    "    vocab_len, max_length, train_X, train_y, val_X, val_y, test_X, test_y = vectorize_embeddings(train, val, test, corpus)\n",
    "    model = hyb_glove_model(corpus, max_length, vocab_len)\n",
    "    predictions = NN_predict2(train_X, train_y, val_X, val_y, test_X, test_y, model)\n",
    "    results, matrix = print_results(\"C-LSTM - GloVe\", predictions, test_y)  \n",
    "    return results, matrix\n",
    "\n",
    "#  This method evaluates the C-LSTM classifier, with POS tags\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data. \n",
    "def clstm_pos(train, val, test, corpus):\n",
    "    embedding_dim = 300\n",
    "    vocab_len, max_length, train_X, train_y, val_X, val_y, test_X, test_y = vectorize_embeddings(train, val, test, corpus)\n",
    "    train_POS, val_POS, test_POS = pos_tokenize(train, val, test, corpus, max_length)\n",
    "    model = hyb_model_pos(embedding_dim, max_length, vocab_len)\n",
    "    predictions = NN_predict_pos(train_X, train_POS, train_y, val_X, val_POS, val_y, test_X, test_POS, test_y, model)\n",
    "    results, matrix = print_results(\"C-LSTM - POS\", predictions, test_y)  \n",
    "    return results, matrix\n",
    "\n",
    "#  This method evaluates the C-LSTM classifier, with Glove and POS tags\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data. \n",
    "def clstm_glove_pos(train, val, test, corpus):\n",
    "    vocab_len, max_length, train_X, train_y, val_X, val_y, test_X, test_y = vectorize_embeddings(train, val, test, corpus)\n",
    "    train_POS, val_POS, test_POS = pos_tokenize(train, val, test, corpus, max_length)\n",
    "    model = hyb_glove_model_pos(corpus, max_length, vocab_len)\n",
    "    predictions = NN_predict_pos(train_X, train_POS, train_y, val_X, val_POS, val_y, test_X, test_POS, test_y, model)\n",
    "    results, matrix = print_results(\"C-LSTM - GloVe & POS\", predictions, test_y)  \n",
    "    return results, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classifier\n",
    "- (LSTM + CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Classifier - LSTM + CNN\n",
    "def combined_model(max_length, embedding_dim, vocab_len):\n",
    "    inp = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(input_dim=vocab_len, \n",
    "                               output_dim=embedding_dim, \n",
    "                               input_length=max_length,\n",
    "                                 trainable=True)(inp)\n",
    "    \n",
    "    x = emb_word\n",
    "    y = x\n",
    "    \n",
    "    x = layers.Conv1D(120, 9, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(80, 9, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    y = layers.LSTM(units = 150, return_sequences = True)(y)\n",
    "    y = layers.MaxPooling1D(2)(y)\n",
    "    y = layers.LSTM(units = 150, return_sequences = True)(y)\n",
    "    y = layers.GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    z = Concatenate()([x,y])\n",
    "    out = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=out)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "# Ensemble Classifier - LSTM + CNN & Glove\n",
    "def combined_glove_model(embedding_matrix, max_length, embedding_dim, vocab_len):\n",
    "    inp = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, \n",
    "                              weights=[embedding_matrix], input_length=max_length, trainable=False)(inp)\n",
    "    \n",
    "    x = emb_word\n",
    "    y = x\n",
    "    \n",
    "    x = layers.Conv1D(120, 9, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(80, 9, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    y = layers.LSTM(units = 150, return_sequences = True)(y)\n",
    "    y = layers.MaxPooling1D(2)(y)\n",
    "    y = layers.LSTM(units = 150, return_sequences = True)(y)\n",
    "    y = layers.GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    z = Concatenate()([x,y])\n",
    "    out = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=out)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "# Ensemble Classifier - LSTM + CNN & POS tags\n",
    "def combined_pos_model(max_length, embedding_dim, vocab_len):\n",
    "    inp = Input(shape=(100,))\n",
    "    pos = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=False)(inp)\n",
    "    \n",
    "    pos_in = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=True)(pos)\n",
    "    x = Concatenate()([emb_word,pos_in])\n",
    "    y = x\n",
    "    \n",
    "    x = layers.Conv1D(120, 9, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(80, 9, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    y = layers.LSTM(units = 150, return_sequences = True)(y)\n",
    "    y = layers.MaxPooling1D(2)(y)\n",
    "    y = layers.LSTM(units = 150, return_sequences = True)(y)\n",
    "    y = layers.GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    z = Concatenate()([x,y])\n",
    "    out = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs=[inp, pos], outputs=out)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "# Ensemble Classifier - LSTM + CNN & Glove + POS tags\n",
    "def combined_glove_pos_model(embedding_matrix, max_length, embedding_dim, vocab_len):\n",
    "    inp = Input(shape=(100,))\n",
    "    pos = Input(shape=(100,))\n",
    "\n",
    "    emb_word = layers.Embedding(vocab_len, embedding_dim, \n",
    "                              weights=[embedding_matrix], input_length=max_length, trainable=False)(inp)\n",
    "    \n",
    "    pos_in = layers.Embedding(vocab_len, embedding_dim, input_length=max_length, trainable=True)(pos)\n",
    "    x = Concatenate()([emb_word,pos_in])\n",
    "    y = x\n",
    "    \n",
    "    x = layers.Conv1D(120, 9, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(80, 9, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    y = layers.LSTM(units = 150, return_sequences = True)(y)\n",
    "    y = layers.MaxPooling1D(2)(y)\n",
    "    y = layers.LSTM(units = 150, return_sequences = True)(y)\n",
    "    y = layers.GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    z = Concatenate()([x,y])\n",
    "    out = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs=[inp, pos], outputs=out)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates glove embedding matrix & returns the model\n",
    "def com_glove_model(corpus, max_length, vocab_len):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    model = combined_glove_model(embedding_matrix, max_length, embedding_dim, vocab_len)\n",
    "    return model\n",
    "\n",
    "# fetches the model\n",
    "def com_model(embedding_dim, max_length, vocab_len):\n",
    "    model = combined_model(max_length, embedding_dim, vocab_len)\n",
    "    return model\n",
    "\n",
    "# creates the glove embedding matrix & returns the model (POS)\n",
    "def com_model_pos(embedding_dim, max_length, vocab_len):\n",
    "    model = combined_pos_model(max_length, embedding_dim, vocab_len)\n",
    "    return model\n",
    "\n",
    "# fetches the model (POS)\n",
    "def com_glove_model_pos(corpus, max_length, vocab_len):\n",
    "    embedding_dim, embedding_matrix = create_glove_matrix(corpus)\n",
    "    model = combined_glove_pos_model(embedding_matrix, max_length, embedding_dim, vocab_len)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This method evaluates the Ensemble classifier, \n",
    "# It takes in a split dataset but returns evaluation metrics from the test data. \n",
    "def ensemble_classifier(train, val, test, corpus):\n",
    "    embedding_dim = 300\n",
    "    vocab_len, max_length, train_X, train_y, val_X, val_y, test_X, test_y = vectorize_embeddings(train, val, test, corpus)\n",
    "    model = com_model(embedding_dim, max_length, vocab_len)\n",
    "    predictions = NN_predict2(train_X, train_y, val_X, val_y, test_X, test_y, model)\n",
    "    results, matrix = print_results(\"Ensemble Classifier\", predictions, test_y)  \n",
    "    return results, matrix\n",
    "\n",
    "#  This method evaluates the Ensemble classifier, with Glove \n",
    "# It takes in a split dataset but returns evaluation metrics from the test data. \n",
    "def ensemble_classifier_glove(train, val, test, corpus):\n",
    "    vocab_len, max_length, train_X, train_y, val_X, val_y, test_X, test_y = vectorize_embeddings(train, val, test, corpus)\n",
    "    model = com_glove_model(corpus, max_length, vocab_len)\n",
    "    predictions = NN_predict2(train_X, train_y, val_X, val_y, test_X, test_y, model)\n",
    "    results, matrix = print_results(\"Ensemble Classifier - GloVe\", predictions, test_y)  \n",
    "    return results, matrix\n",
    "\n",
    "#  This method evaluates the Ensemble classifier, with POS tags\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data. \n",
    "def ensemble_classifier_pos(train, val, test, corpus):\n",
    "    embedding_dim = 300\n",
    "    vocab_len, max_length, train_X, train_y, val_X, val_y, test_X, test_y = vectorize_embeddings(train, val, test, corpus)\n",
    "    train_POS, val_POS, test_POS = pos_tokenize(train, val, test, corpus, max_length)\n",
    "    model = com_model_pos(embedding_dim, max_length, vocab_len)\n",
    "    predictions = NN_predict_pos(train_X, train_POS, train_y, val_X, val_POS, val_y, test_X, test_POS, test_y, model)\n",
    "    results, matrix = print_results(\"Ensemble Classifier - POS tags\", predictions, test_y)  \n",
    "    return results, matrix\n",
    "\n",
    "#  This method evaluates the Ensemble classifier, with Glove and POS tags\n",
    "# It takes in a split dataset but returns evaluation metrics from the test data. \n",
    "def ensemble_classifier_glove_pos(train, val, test, corpus):\n",
    "    embedding_dim = 300\n",
    "    vocab_len, max_length, train_X, train_y, val_X, val_y, test_X, test_y = vectorize_embeddings(train, val, test, corpus)\n",
    "    train_POS, val_POS, test_POS = pos_tokenize(train, val, test, corpus, max_length)\n",
    "    model = com_glove_model_pos(corpus, max_length, vocab_len)\n",
    "    predictions = NN_predict_pos(train_X, train_POS, train_y, val_X, val_POS, val_y, test_X, test_POS, test_y, model)\n",
    "    results, matrix = print_results(\"Ensemble Classifier - Glove & POS\", predictions, test_y)  \n",
    "    return results, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment01: TF-IDF weighting experiment\n",
    "- Logistic Regression\n",
    "- Support Vector Machines\n",
    "\n",
    "Compares the use of ngram vectors to similar vectors that have tf-idf weighting applied on both dataset A and dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment part 1: dataset A\n",
    "def tf_idf_experiment_A():\n",
    "    train, val, test, corpus = import_dataset_A()\n",
    "    lr_results, lr_matrix = logistic_regression(train, val, test, corpus)\n",
    "    print()\n",
    "    lr_results2, lr_matrix2 = logistic_regression_tfidf(train, val, test, corpus)\n",
    "    print()\n",
    "    svm_results, svm_matrix = support_vector_machine(train, val, test, corpus)\n",
    "    print()\n",
    "    svm_results2, svm_matrix2 = support_vector_machine_tfidf(train, val, test, corpus)\n",
    "    print()\n",
    "    results = pd.concat([lr_results, lr_results2, svm_results, svm_results2])\n",
    "    matrix = [lr_matrix,lr_matrix2,svm_matrix,svm_matrix2]\n",
    "    return results, matrix\n",
    "\n",
    "# Experiment part 2: dataset B\n",
    "def tf_idf_experiment_B():\n",
    "    train, val, test, corpus = import_dataset_B()\n",
    "    lr_results, lr_matrix = logistic_regression(train, val, test, corpus)\n",
    "    print()\n",
    "    lr_results2, lr_matrix2 = logistic_regression_tfidf(train, val, test, corpus)\n",
    "    print()\n",
    "    svm_results, svm_matrix = support_vector_machine(train, val, test, corpus)\n",
    "    print()\n",
    "    svm_results2, svm_matrix2 = support_vector_machine_tfidf(train, val, test, corpus)\n",
    "    print()\n",
    "    results = pd.concat([lr_results, lr_results2, svm_results, svm_results2])\n",
    "    matrix = [lr_matrix,lr_matrix2,svm_matrix,svm_matrix2]\n",
    "    return results, matrix\n",
    "\n",
    "# This method performs both parts of the experiment and returns results.\n",
    "def tf_idf_experiment():\n",
    "    print(\"TF-IDF Weighting Experiment...\")\n",
    "    print()\n",
    "    print(\"Datatset A:\")\n",
    "    print()\n",
    "    results_A, matrix_A = tf_idf_experiment_A()\n",
    "    print(\"--------------------------------------------\")\n",
    "    print()\n",
    "    print(\"Dataset B:\")\n",
    "    print()\n",
    "    results_B, matrix_B = tf_idf_experiment_B()\n",
    "    print()\n",
    "    return results_A, matrix_A, results_B, matrix_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Weighting Experiment...\n",
      "\n",
      "Datatset A:\n",
      "\n",
      "Logistic Regression...\n",
      "Training-Accuracy:0.9955572876071707\n",
      "Validation-Accuracy:0.7584459459459459\n",
      "\n",
      "Confusion matrix:\n",
      "[[717  29]\n",
      " [ 32  55]]\n",
      "            Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Logistic Regression  0.926771   0.654762  0.632184  0.643275\n",
      "\n",
      "Logistic Regression...\n",
      "Training-Accuracy:0.9829306313328137\n",
      "Validation-Accuracy:0.7618243243243243\n",
      "\n",
      "Confusion matrix:\n",
      "[[705  41]\n",
      " [ 32  55]]\n",
      "                     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Logistic Regression (TF-IDF)  0.912365   0.572917  0.632184  0.601093\n",
      "\n",
      "Support Vector Machine...\n",
      "Training-Accuracy:0.7247856586126267\n",
      "Validation-Accuracy:0.6672297297297297\n",
      "\n",
      "Confusion matrix:\n",
      "[[590 156]\n",
      " [ 32  55]]\n",
      "               Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Support Vector Machine   0.77431   0.260664  0.632184  0.369128\n",
      "\n",
      "Support Vector Machine...\n",
      "Training-Accuracy:0.8214341387373344\n",
      "Validation-Accuracy:0.7449324324324325\n",
      "\n",
      "Confusion matrix:\n",
      "[[646 100]\n",
      " [ 30  57]]\n",
      "                        Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Support Vector Machine (TF-IDF)  0.843938   0.363057  0.655172  0.467213\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Dataset B:\n",
      "\n",
      "Logistic Regression...\n",
      "Training-Accuracy:0.9562743569758378\n",
      "Validation-Accuracy:0.5829207920792079\n",
      "\n",
      "Confusion matrix:\n",
      "[[452  24]\n",
      " [293  55]]\n",
      "            Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Logistic Regression  0.615291   0.696203  0.158046  0.257611\n",
      "\n",
      "Logistic Regression...\n",
      "Training-Accuracy:0.9275915822291504\n",
      "Validation-Accuracy:0.5185643564356436\n",
      "\n",
      "Confusion matrix:\n",
      "[[473   3]\n",
      " [337  11]]\n",
      "                     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Logistic Regression (TF-IDF)  0.587379   0.785714  0.031609  0.060773\n",
      "\n",
      "Support Vector Machine...\n",
      "Training-Accuracy:0.7225253312548714\n",
      "Validation-Accuracy:0.6113861386138614\n",
      "\n",
      "Confusion matrix:\n",
      "[[424  52]\n",
      " [250  98]]\n",
      "               Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Support Vector Machine  0.633495   0.653333  0.281609  0.393574\n",
      "\n",
      "Support Vector Machine...\n",
      "Training-Accuracy:0.8038191738113796\n",
      "Validation-Accuracy:0.5284653465346535\n",
      "\n",
      "Confusion matrix:\n",
      "[[474   2]\n",
      " [331  17]]\n",
      "                        Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Support Vector Machine (TF-IDF)  0.595874   0.894737  0.048851  0.092643\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to run the experiment\n",
    "# This particular cell has a runtime of around 1 min.\n",
    "results_A, matrix_A, results_B, matrix_B = tf_idf_experiment()\n",
    "results_A.to_pickle(\"results_A.pkl\")\n",
    "results_B.to_pickle(\"results_B.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment02:\n",
    "Pre-processing techniques experiment\n",
    "- LSTM\n",
    "- CNN\n",
    "- C-LSTM\n",
    "- Ensemble (CNN + LSTM)\n",
    "\n",
    "Compares the use of stop word removal and lemmatization on neural network classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This particular method runs all of the neural network classifiers in order\n",
    "def nn_classify(train, val, test, corpus):\n",
    "    results1, matrix1 = lstm(train, val, test, corpus)\n",
    "    print()\n",
    "    results2, matrix2 = lstm_glove(train, val, test, corpus)\n",
    "    print()\n",
    "    results3, matrix3 = cnn(train, val, test, corpus)\n",
    "    print()\n",
    "    results4, matrix4 = cnn_glove(train, val, test, corpus)\n",
    "    print()\n",
    "    results5, matrix5 = clstm(train, val, test, corpus)\n",
    "    print()\n",
    "    results6, matrix6 = clstm_glove(train, val, test, corpus)\n",
    "    print()\n",
    "    results7, matrix7 = ensemble_classifier(train, val, test, corpus)\n",
    "    print()\n",
    "    results8, matrix8 = ensemble_classifier_glove(train, val, test, corpus)\n",
    "    print()\n",
    "    results = pd.concat([results1, results2, results3, results4, results5, results6, results7, results8])\n",
    "    matrix = [matrix1, matrix2, matrix3, matrix4, matrix5, matrix6, matrix7, matrix8]\n",
    "    return results, matrix\n",
    "\n",
    "# This particular method runs all of the neural network classifiers with pos tags as an input \n",
    "def nn_classify_pos(train, val, test, corpus):\n",
    "    results1, matrix1 = lstm_pos(train, val, test, corpus)\n",
    "    print()\n",
    "    results2, matrix2 = lstm_glove_pos(train, val, test, corpus)\n",
    "    print()\n",
    "    results3, matrix3 = cnn_pos(train, val, test, corpus)\n",
    "    print()\n",
    "    results4, matrix4 = cnn_glove_pos(train, val, test, corpus)   \n",
    "    print()\n",
    "    results5, matrix5 = clstm_pos(train, val, test, corpus)\n",
    "    print()\n",
    "    results6, matrix6 = clstm_glove_pos(train, val, test, corpus)\n",
    "    print()\n",
    "    results7, matrix7 = ensemble_classifier_pos(train, val, test, corpus)\n",
    "    print()\n",
    "    results8, matrix8 = ensemble_classifier_glove_pos(train, val, test, corpus)\n",
    "    print()\n",
    "    results = pd.concat([results1, results2, results3, results4, results5, results6, results7, results8])\n",
    "    matrix = [matrix1, matrix2, matrix3, matrix4, matrix5, matrix6, matrix7, matrix8]\n",
    "    return results, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment part 1: dataset A\n",
    "def pp_experiment_A():\n",
    "    train, val, test, corpus = import_dataset_A()\n",
    "    results1, matrix1 = nn_classify(train, val, test, corpus)\n",
    "    train, val, test, corpus = import_dataset_A_swl()\n",
    "    results2, matrix2 = nn_classify(train, val, test, corpus)\n",
    "    return results1, results2, matrix1, matrix2\n",
    "\n",
    "# Experiment part 2: dataset B\n",
    "def pp_experiment_B():\n",
    "    train, val, test, corpus = import_dataset_B()\n",
    "    results1, matrix1 = nn_classify(train, val, test, corpus)\n",
    "    train, val, test, corpus = import_dataset_B_swl()\n",
    "    results2, matrix2 = nn_classify(train, val, test, corpus)\n",
    "    return results1, results2, matrix1, matrix2\n",
    "\n",
    "# This method performs both parts of the experiment and returns results.\n",
    "def pp_experiment():\n",
    "    print(\"Pre-processing techniques experiment...\")\n",
    "    print()\n",
    "    print(\"Dataset A:\")\n",
    "    print()\n",
    "    results1, results2, matrix1, matrix2 = pp_experiment_A()\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print()\n",
    "    print(\"Dataset B:\")\n",
    "    print()\n",
    "    results3, results4, matrix3, matrix4 = pp_experiment_B()\n",
    "    return results1, results2, matrix1, matrix2, results3, results4, matrix3, matrix4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing techniques experiment...\n",
      "\n",
      "Dataset A:\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 52ms/step\n",
      "training data...\n",
      "Accuracy: 0.9272798299789429\n",
      "Loss:0.2389167696237564\n",
      "validation data...\n",
      "Accuracy: 0.7668918967247009\n",
      "Loss:0.5746933221817017\n",
      "\n",
      "Confusion matrix:\n",
      "[[695  51]\n",
      " [ 29  58]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0       LSTM  0.903962    0.53211  0.666667  0.591837\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 1s 73ms/step\n",
      "training data...\n",
      "Accuracy: 0.7868277430534363\n",
      "Loss:0.5217134356498718\n",
      "validation data...\n",
      "Accuracy: 0.7297297120094299\n",
      "Loss:0.6893956661224365\n",
      "\n",
      "Confusion matrix:\n",
      "[[674  72]\n",
      " [ 36  51]]\n",
      "     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM - Glove  0.870348   0.414634  0.586207  0.485714\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "training data...\n",
      "Accuracy: 0.9525331258773804\n",
      "Loss:0.15320146083831787\n",
      "validation data...\n",
      "Accuracy: 0.7347972989082336\n",
      "Loss:0.6285487413406372\n",
      "\n",
      "Confusion matrix:\n",
      "[[656  90]\n",
      " [ 25  62]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0        CNN  0.861945   0.407895  0.712644  0.518828\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 0s 12ms/step\n",
      "training data...\n",
      "Accuracy: 0.9512081146240234\n",
      "Loss:0.15604814887046814\n",
      "validation data...\n",
      "Accuracy: 0.7533783912658691\n",
      "Loss:0.5416519641876221\n",
      "\n",
      "Confusion matrix:\n",
      "[[666  80]\n",
      " [ 23  64]]\n",
      "    Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  CNN - Glove  0.876351   0.444444  0.735632  0.554113\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 43ms/step\n",
      "training data...\n",
      "Accuracy: 0.9766172766685486\n",
      "Loss:0.09523577243089676\n",
      "validation data...\n",
      "Accuracy: 0.7297297120094299\n",
      "Loss:0.6977335214614868\n",
      "\n",
      "Confusion matrix:\n",
      "[[685  61]\n",
      " [ 33  54]]\n",
      "  Classifier  Accuracy  Precision   Recall   F-Score\n",
      "0     C-LSTM  0.887155   0.469565  0.62069  0.534653\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 45ms/step\n",
      "training data...\n",
      "Accuracy: 0.9367108345031738\n",
      "Loss:0.1876671463251114\n",
      "validation data...\n",
      "Accuracy: 0.7618243098258972\n",
      "Loss:0.5481041073799133\n",
      "\n",
      "Confusion matrix:\n",
      "[[626 120]\n",
      " [ 21  66]]\n",
      "       Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  C-LSTM - GloVe  0.830732   0.354839  0.758621  0.483516\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 83ms/step\n",
      "training data...\n",
      "Accuracy: 0.976773202419281\n",
      "Loss:0.07883931696414948\n",
      "validation data...\n",
      "Accuracy: 0.7263513803482056\n",
      "Loss:0.7840864062309265\n",
      "\n",
      "Confusion matrix:\n",
      "[[710  36]\n",
      " [ 41  46]]\n",
      "            Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier  0.907563   0.560976  0.528736  0.544379\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 84ms/step\n",
      "training data...\n",
      "Accuracy: 0.9671862721443176\n",
      "Loss:0.13304059207439423\n",
      "validation data...\n",
      "Accuracy: 0.7516891956329346\n",
      "Loss:0.5405557751655579\n",
      "\n",
      "Confusion matrix:\n",
      "[[701  45]\n",
      " [ 31  56]]\n",
      "                    Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier - GloVe  0.908764   0.554455  0.643678  0.595745\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 51ms/step\n",
      "training data...\n",
      "Accuracy: 0.9079501032829285\n",
      "Loss:0.27084484696388245\n",
      "validation data...\n",
      "Accuracy: 0.6824324131011963\n",
      "Loss:0.7660654187202454\n",
      "\n",
      "Confusion matrix:\n",
      "[[650  96]\n",
      " [ 32  55]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0       LSTM  0.846339   0.364238  0.632184  0.462185\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 75ms/step\n",
      "training data...\n",
      "Accuracy: 0.8460639119148254\n",
      "Loss:0.37259456515312195\n",
      "validation data...\n",
      "Accuracy: 0.7263513803482056\n",
      "Loss:0.6015699505805969\n",
      "\n",
      "Confusion matrix:\n",
      "[[584 162]\n",
      " [ 24  63]]\n",
      "     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM - Glove  0.776711       0.28  0.724138  0.403846\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "training data...\n",
      "Accuracy: 0.9383476376533508\n",
      "Loss:0.20461544394493103\n",
      "validation data...\n",
      "Accuracy: 0.650337815284729\n",
      "Loss:0.7993535995483398\n",
      "\n",
      "Confusion matrix:\n",
      "[[686  60]\n",
      " [ 42  45]]\n",
      "  Classifier  Accuracy  Precision    Recall  F-Score\n",
      "0        CNN  0.877551   0.428571  0.517241  0.46875\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 10ms/step\n",
      "training data...\n",
      "Accuracy: 0.9123928546905518\n",
      "Loss:0.24777445197105408\n",
      "validation data...\n",
      "Accuracy: 0.6689189076423645\n",
      "Loss:0.7131195664405823\n",
      "\n",
      "Confusion matrix:\n",
      "[[679  67]\n",
      " [ 51  36]]\n",
      "    Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  CNN - Glove  0.858343   0.349515  0.413793  0.378947\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 42ms/step\n",
      "training data...\n",
      "Accuracy: 0.961340606212616\n",
      "Loss:0.13981321454048157\n",
      "validation data...\n",
      "Accuracy: 0.6537162065505981\n",
      "Loss:0.8786711692810059\n",
      "\n",
      "Confusion matrix:\n",
      "[[673  73]\n",
      " [ 49  38]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0     C-LSTM  0.853541   0.342342  0.436782  0.383838\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 42ms/step\n",
      "training data...\n",
      "Accuracy: 0.9392829537391663\n",
      "Loss:0.19264547526836395\n",
      "validation data...\n",
      "Accuracy: 0.6283783912658691\n",
      "Loss:0.9348418712615967\n",
      "\n",
      "Confusion matrix:\n",
      "[[694  52]\n",
      " [ 57  30]]\n",
      "       Classifier  Accuracy  Precision    Recall  F-Score\n",
      "0  C-LSTM - GloVe  0.869148   0.365854  0.344828  0.35503\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 81ms/step\n",
      "training data...\n",
      "Accuracy: 0.9726422429084778\n",
      "Loss:0.09351915121078491\n",
      "validation data...\n",
      "Accuracy: 0.6638513803482056\n",
      "Loss:0.878007709980011\n",
      "\n",
      "Confusion matrix:\n",
      "[[648  98]\n",
      " [ 41  46]]\n",
      "            Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier  0.833133   0.319444  0.528736  0.398268\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 84ms/step\n",
      "training data...\n",
      "Accuracy: 0.9636788964271545\n",
      "Loss:0.13085481524467468\n",
      "validation data...\n",
      "Accuracy: 0.7145270109176636\n",
      "Loss:0.6829079389572144\n",
      "\n",
      "Confusion matrix:\n",
      "[[634 112]\n",
      " [ 32  55]]\n",
      "                    Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier - GloVe  0.827131   0.329341  0.632184  0.433071\n",
      "\n",
      "-----------------------------------------------\n",
      "\n",
      "Dataset B:\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 0s 53ms/step\n",
      "training data...\n",
      "Accuracy: 0.8938425779342651\n",
      "Loss:0.3019798994064331\n",
      "validation data...\n",
      "Accuracy: 0.6163366436958313\n",
      "Loss:0.9058281779289246\n",
      "\n",
      "Confusion matrix:\n",
      "[[446  30]\n",
      " [263  85]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0       LSTM  0.644417    0.73913  0.244253  0.367171\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 76ms/step\n",
      "training data...\n",
      "Accuracy: 0.8691348433494568\n",
      "Loss:0.3191322386264801\n",
      "validation data...\n",
      "Accuracy: 0.6757425665855408\n",
      "Loss:0.8005777597427368\n",
      "\n",
      "Confusion matrix:\n",
      "[[447  29]\n",
      " [218 130]]\n",
      "     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM - Glove  0.700243    0.81761  0.373563  0.512821\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "training data...\n",
      "Accuracy: 0.9024941325187683\n",
      "Loss:0.2762417197227478\n",
      "validation data...\n",
      "Accuracy: 0.6349009871482849\n",
      "Loss:0.88558429479599\n",
      "\n",
      "Confusion matrix:\n",
      "[[429  47]\n",
      " [247 101]]\n",
      "  Classifier  Accuracy  Precision   Recall   F-Score\n",
      "0        CNN  0.643204   0.682432  0.29023  0.407258\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 10ms/step\n",
      "training data...\n",
      "Accuracy: 0.893218994140625\n",
      "Loss:0.274004191160202\n",
      "validation data...\n",
      "Accuracy: 0.6547029614448547\n",
      "Loss:0.7061198949813843\n",
      "\n",
      "Confusion matrix:\n",
      "[[426  50]\n",
      " [213 135]]\n",
      "    Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  CNN - Glove  0.680825    0.72973  0.387931  0.506567\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 43ms/step\n",
      "training data...\n",
      "Accuracy: 0.9155105352401733\n",
      "Loss:0.2414519041776657\n",
      "validation data...\n",
      "Accuracy: 0.5903465151786804\n",
      "Loss:1.1308529376983643\n",
      "\n",
      "Confusion matrix:\n",
      "[[449  27]\n",
      " [277  71]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0     C-LSTM  0.631068    0.72449  0.204023  0.318386\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 42ms/step\n",
      "training data...\n",
      "Accuracy: 0.9281371831893921\n",
      "Loss:0.20665740966796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data...\n",
      "Accuracy: 0.6881188154220581\n",
      "Loss:0.7246740460395813\n",
      "\n",
      "Confusion matrix:\n",
      "[[399  77]\n",
      " [180 168]]\n",
      "       Classifier  Accuracy  Precision    Recall  F-Score\n",
      "0  C-LSTM - GloVe  0.688107   0.685714  0.482759  0.56661\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 78ms/step\n",
      "training data...\n",
      "Accuracy: 0.9353078603744507\n",
      "Loss:0.1833030879497528\n",
      "validation data...\n",
      "Accuracy: 0.594059407711029\n",
      "Loss:1.1916375160217285\n",
      "\n",
      "Confusion matrix:\n",
      "[[456  20]\n",
      " [283  65]]\n",
      "            Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier  0.632282   0.764706  0.186782  0.300231\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 79ms/step\n",
      "training data...\n",
      "Accuracy: 0.9236944913864136\n",
      "Loss:0.2020927369594574\n",
      "validation data...\n",
      "Accuracy: 0.6076732873916626\n",
      "Loss:1.0626261234283447\n",
      "\n",
      "Confusion matrix:\n",
      "[[465  11]\n",
      " [280  68]]\n",
      "                    Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier - GloVe  0.646845   0.860759  0.195402  0.318501\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 51ms/step\n",
      "training data...\n",
      "Accuracy: 0.8533905148506165\n",
      "Loss:0.38975638151168823\n",
      "validation data...\n",
      "Accuracy: 0.6188119053840637\n",
      "Loss:0.8095121383666992\n",
      "\n",
      "Confusion matrix:\n",
      "[[379  97]\n",
      " [204 144]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0       LSTM  0.634709    0.59751  0.413793  0.488964\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 75ms/step\n",
      "training data...\n",
      "Accuracy: 0.8190179467201233\n",
      "Loss:0.4221252202987671\n",
      "validation data...\n",
      "Accuracy: 0.6819307208061218\n",
      "Loss:0.607688307762146\n",
      "\n",
      "Confusion matrix:\n",
      "[[378  98]\n",
      " [142 206]]\n",
      "     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM - Glove  0.708738   0.677632  0.591954  0.631902\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "training data...\n",
      "Accuracy: 0.8698363304138184\n",
      "Loss:0.3595830202102661\n",
      "validation data...\n",
      "Accuracy: 0.6089109182357788\n",
      "Loss:0.7090504169464111\n",
      "\n",
      "Confusion matrix:\n",
      "[[360 116]\n",
      " [224 124]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0        CNN  0.587379   0.516667  0.356322  0.421769\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 10ms/step\n",
      "training data...\n",
      "Accuracy: 0.8744348883628845\n",
      "Loss:0.3227606415748596\n",
      "validation data...\n",
      "Accuracy: 0.676980197429657\n",
      "Loss:0.6388213038444519\n",
      "\n",
      "Confusion matrix:\n",
      "[[268 208]\n",
      " [ 73 275]]\n",
      "    Classifier  Accuracy  Precision   Recall   F-Score\n",
      "0  CNN - Glove  0.658981   0.569358  0.79023  0.661853\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 41ms/step\n",
      "training data...\n",
      "Accuracy: 0.8544816970825195\n",
      "Loss:0.352446049451828\n",
      "validation data...\n",
      "Accuracy: 0.5853960514068604\n",
      "Loss:0.9741194248199463\n",
      "\n",
      "Confusion matrix:\n",
      "[[363 113]\n",
      " [222 126]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0     C-LSTM  0.593447   0.527197  0.362069  0.429302\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 41ms/step\n",
      "training data...\n",
      "Accuracy: 0.8992205858230591\n",
      "Loss:0.28293073177337646\n",
      "validation data...\n",
      "Accuracy: 0.6608911156654358\n",
      "Loss:0.6724443435668945\n",
      "\n",
      "Confusion matrix:\n",
      "[[305 171]\n",
      " [128 220]]\n",
      "       Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  C-LSTM - GloVe  0.637136    0.56266  0.632184  0.595399\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 80ms/step\n",
      "training data...\n",
      "Accuracy: 0.898129403591156\n",
      "Loss:0.27273866534233093\n",
      "validation data...\n",
      "Accuracy: 0.5358911156654358\n",
      "Loss:1.1259862184524536\n",
      "\n",
      "Confusion matrix:\n",
      "[[457  19]\n",
      " [314  34]]\n",
      "            Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier  0.595874   0.641509  0.097701  0.169576\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 1s 79ms/step\n",
      "training data...\n",
      "Accuracy: 0.9544037580490112\n",
      "Loss:0.14370931684970856\n",
      "validation data...\n",
      "Accuracy: 0.5680692791938782\n",
      "Loss:1.1966828107833862\n",
      "\n",
      "Confusion matrix:\n",
      "[[426  50]\n",
      " [256  92]]\n",
      "                    Classifier  Accuracy  Precision    Recall  F-Score\n",
      "0  Ensemble Classifier - GloVe  0.628641   0.647887  0.264368  0.37551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to run the experiment\n",
    "# This particular cell has a runtime of around 20-25 min.\n",
    "results1, results2, matrix1, matrix2, results3, results4, matrix3, matrix4 = pp_experiment()\n",
    "results1.to_pickle(\"results_1.pkl\")\n",
    "results2.to_pickle(\"results_2.pkl\")\n",
    "results3.to_pickle(\"results_3.pkl\")\n",
    "results4.to_pickle(\"results_4.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment03:\n",
    "Data Augmentation experiment\n",
    "- LSTM\n",
    "- CNN\n",
    "- C-LSTM\n",
    "- Ensemble (CNN + LSTM)\n",
    "\n",
    "Compares the use of augmented training data on neural network classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment part 1: dataset A\n",
    "def aug_experiment_A(): \n",
    "    train, val, test, corpus = import_dataset_A_aug()\n",
    "    results, matrix = nn_classify(train, val, test, corpus)\n",
    "    return results, matrix\n",
    "\n",
    "# Experiment part 2: dataset B\n",
    "def aug_experiment_B():\n",
    "    train, val, test, corpus = import_dataset_B_aug()\n",
    "    results, matrix = nn_classify(train, val, test, corpus)\n",
    "    return results, matrix\n",
    "\n",
    "# This method performs both parts of the experiment and returns results.\n",
    "def aug_experiment():\n",
    "    print(\"Data Augmentation experiment...\")\n",
    "    print()\n",
    "    print(\"Dataset A:\")\n",
    "    print()\n",
    "    results1, matrix1 = aug_experiment_A()\n",
    "    print(\"------------------------------------------------\")\n",
    "    print()\n",
    "    print(\"Dataset B:\")\n",
    "    print()\n",
    "    results2, matrix2 = aug_experiment_B()\n",
    "    return results1, results2, matrix1, matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Augmentation experiment...\n",
      "\n",
      "Dataset A:\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 50ms/step\n",
      "training data...\n",
      "Accuracy: 0.8665627241134644\n",
      "Loss:0.34305328130722046\n",
      "validation data...\n",
      "Accuracy: 0.7432432174682617\n",
      "Loss:0.5933619141578674\n",
      "\n",
      "Confusion matrix:\n",
      "[[697  49]\n",
      " [ 35  52]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0       LSTM   0.89916   0.514851  0.597701  0.553191\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 1s 83ms/step\n",
      "training data...\n",
      "Accuracy: 0.8825019598007202\n",
      "Loss:0.28956174850463867\n",
      "validation data...\n",
      "Accuracy: 0.8091216087341309\n",
      "Loss:0.4254849851131439\n",
      "\n",
      "Confusion matrix:\n",
      "[[675  71]\n",
      " [ 20  67]]\n",
      "     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM - Glove  0.890756   0.485507  0.770115  0.595556\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "training data...\n",
      "Accuracy: 0.9438425302505493\n",
      "Loss:0.1699533611536026\n",
      "validation data...\n",
      "Accuracy: 0.75\n",
      "Loss:0.6121662855148315\n",
      "\n",
      "Confusion matrix:\n",
      "[[638 108]\n",
      " [ 23  64]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0        CNN  0.842737   0.372093  0.735632  0.494208\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 10ms/step\n",
      "training data...\n",
      "Accuracy: 0.8882696628570557\n",
      "Loss:0.29020974040031433\n",
      "validation data...\n",
      "Accuracy: 0.7635135054588318\n",
      "Loss:0.5392538905143738\n",
      "\n",
      "Confusion matrix:\n",
      "[[668  78]\n",
      " [ 23  64]]\n",
      "    Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  CNN - Glove  0.878752   0.450704  0.735632  0.558952\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 41ms/step\n",
      "training data...\n",
      "Accuracy: 0.9531956315040588\n",
      "Loss:0.1441739797592163\n",
      "validation data...\n",
      "Accuracy: 0.7550675868988037\n",
      "Loss:0.5855857133865356\n",
      "\n",
      "Confusion matrix:\n",
      "[[643 103]\n",
      " [ 21  66]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0     C-LSTM   0.85114   0.390533  0.758621  0.515625\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 41ms/step\n",
      "training data...\n",
      "Accuracy: 0.9556118249893188\n",
      "Loss:0.15492822229862213\n",
      "validation data...\n",
      "Accuracy: 0.775337815284729\n",
      "Loss:0.5075536370277405\n",
      "\n",
      "Confusion matrix:\n",
      "[[637 109]\n",
      " [ 16  71]]\n",
      "       Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  C-LSTM - GloVe   0.84994   0.394444  0.816092  0.531835\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 80ms/step\n",
      "training data...\n",
      "Accuracy: 0.9672252535820007\n",
      "Loss:0.0974992960691452\n",
      "validation data...\n",
      "Accuracy: 0.7094594836235046\n",
      "Loss:0.7905553579330444\n",
      "\n",
      "Confusion matrix:\n",
      "[[693  53]\n",
      " [ 42  45]]\n",
      "            Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier  0.885954   0.459184  0.517241  0.486486\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 79ms/step\n",
      "training data...\n",
      "Accuracy: 0.9414653182029724\n",
      "Loss:0.17484141886234283\n",
      "validation data...\n",
      "Accuracy: 0.7972972989082336\n",
      "Loss:0.4528501331806183\n",
      "\n",
      "Confusion matrix:\n",
      "[[648  98]\n",
      " [ 13  74]]\n",
      "                    Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier - GloVe  0.866747   0.430233  0.850575  0.571429\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Dataset B:\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 0s 49ms/step\n",
      "training data...\n",
      "Accuracy: 0.8411145806312561\n",
      "Loss:0.37198951840400696\n",
      "validation data...\n",
      "Accuracy: 0.6262376308441162\n",
      "Loss:0.8296158909797668\n",
      "\n",
      "Confusion matrix:\n",
      "[[455  21]\n",
      " [246 102]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0       LSTM  0.675971   0.829268  0.293103  0.433121\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 77ms/step\n",
      "training data...\n",
      "Accuracy: 0.8541699051856995\n",
      "Loss:0.3403380811214447\n",
      "validation data...\n",
      "Accuracy: 0.6398515105247498\n",
      "Loss:0.7791118621826172\n",
      "\n",
      "Confusion matrix:\n",
      "[[456  20]\n",
      " [244 104]]\n",
      "     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM - Glove  0.679612    0.83871  0.298851  0.440678\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "training data...\n",
      "Accuracy: 0.8865159749984741\n",
      "Loss:0.2931501567363739\n",
      "validation data...\n",
      "Accuracy: 0.646039605140686\n",
      "Loss:0.8944719433784485\n",
      "\n",
      "Confusion matrix:\n",
      "[[433  43]\n",
      " [243 105]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0        CNN  0.652913   0.709459  0.301724  0.423387\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 0s 11ms/step\n",
      "training data...\n",
      "Accuracy: 0.8945050835609436\n",
      "Loss:0.2913556694984436\n",
      "validation data...\n",
      "Accuracy: 0.6212871074676514\n",
      "Loss:0.8870570659637451\n",
      "\n",
      "Confusion matrix:\n",
      "[[447  29]\n",
      " [262  86]]\n",
      "    Classifier  Accuracy  Precision    Recall  F-Score\n",
      "0  CNN - Glove  0.646845   0.747826  0.247126  0.37149\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 43ms/step\n",
      "training data...\n",
      "Accuracy: 0.8810989856719971\n",
      "Loss:0.3048000931739807\n",
      "validation data...\n",
      "Accuracy: 0.6262376308441162\n",
      "Loss:0.9641774892807007\n",
      "\n",
      "Confusion matrix:\n",
      "[[416  60]\n",
      " [249  99]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0     C-LSTM     0.625   0.622642  0.284483  0.390533\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 41ms/step\n",
      "training data...\n",
      "Accuracy: 0.9041699171066284\n",
      "Loss:0.2581811845302582\n",
      "validation data...\n",
      "Accuracy: 0.6336633563041687\n",
      "Loss:0.8269208669662476\n",
      "\n",
      "Confusion matrix:\n",
      "[[441  35]\n",
      " [228 120]]\n",
      "       Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  C-LSTM - GloVe  0.680825   0.774194  0.344828  0.477137\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 1s 81ms/step\n",
      "training data...\n",
      "Accuracy: 0.9533904790878296\n",
      "Loss:0.13817384839057922\n",
      "validation data...\n",
      "Accuracy: 0.5990098714828491\n",
      "Loss:1.4621365070343018\n",
      "\n",
      "Confusion matrix:\n",
      "[[448  28]\n",
      " [275  73]]\n",
      "            Classifier  Accuracy  Precision   Recall   F-Score\n",
      "0  Ensemble Classifier  0.632282   0.722772  0.20977  0.325167\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 1s 86ms/step\n",
      "training data...\n",
      "Accuracy: 0.9393218755722046\n",
      "Loss:0.16789668798446655\n",
      "validation data...\n",
      "Accuracy: 0.728960394859314\n",
      "Loss:0.6317850947380066\n",
      "\n",
      "Confusion matrix:\n",
      "[[413  63]\n",
      " [159 189]]\n",
      "                    Classifier  Accuracy  Precision    Recall  F-Score\n",
      "0  Ensemble Classifier - GloVe  0.730583       0.75  0.543103     0.63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to run the experiment\n",
    "# This particular cell has a runtime of around 10-15 min.\n",
    "results5, results6, matrix5, matrix6 = aug_experiment()\n",
    "results5.to_pickle(\"results_5.pkl\")\n",
    "results6.to_pickle(\"results_6.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment04:\n",
    "Part of Speech tags experiment\n",
    "- LSTM\n",
    "- CNN\n",
    "- C-LSTM\n",
    "- Ensemble (CNN + LSTM)\n",
    "\n",
    "Compares the use of POS tags when used with neural network classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment part 1: dataset A\n",
    "def pos_experiment_A():\n",
    "    train, val, test, corpus = import_dataset_A()\n",
    "    results, matrix = nn_classify_pos(train, val, test, corpus)\n",
    "    return results, matrix\n",
    "\n",
    "# Experiment part 2: dataset B\n",
    "def pos_experiment_B():\n",
    "    train, val, test, corpus = import_dataset_B()\n",
    "    results, matrix = nn_classify_pos(train, val, test, corpus)\n",
    "    return results, matrix\n",
    "\n",
    "# This method performs both parts of the experiment and returns results.\n",
    "def pos_experiment():\n",
    "    print(\"Part of speech tags experiment...\")\n",
    "    print()\n",
    "    print(\"Dataset A:\")\n",
    "    print()\n",
    "    results1, matrix1, = pos_experiment_A()\n",
    "    print(\"------------------------------------------------\")\n",
    "    print()\n",
    "    print(\"Dataset B:\")\n",
    "    print()\n",
    "    results2, matrix2  = pos_experiment_B()\n",
    "    return results1, results2, matrix1, matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of speech tags experiment...\n",
      "\n",
      "Dataset A:\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 0s 31ms/step\n",
      "training data...\n",
      "Accuracy: 0.8710833787918091\n",
      "Loss:0.355301171541214\n",
      "validation data...\n",
      "Accuracy: 0.7060810923576355\n",
      "Loss:0.6345945596694946\n",
      "\n",
      "Confusion matrix:\n",
      "[[656  90]\n",
      " [ 41  46]]\n",
      "   Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM - POS  0.842737   0.338235  0.528736  0.412556\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 71ms/step\n",
      "training data...\n",
      "Accuracy: 0.9526110887527466\n",
      "Loss:0.1420421451330185\n",
      "validation data...\n",
      "Accuracy: 0.712837815284729\n",
      "Loss:0.8599256873130798\n",
      "\n",
      "Confusion matrix:\n",
      "[[724  22]\n",
      " [ 46  41]]\n",
      "         Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM Glove & POS  0.918367   0.650794  0.471264  0.546667\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 0s 32ms/step\n",
      "training data...\n",
      "Accuracy: 0.8444271087646484\n",
      "Loss:0.39387041330337524\n",
      "validation data...\n",
      "Accuracy: 0.6587837934494019\n",
      "Loss:0.6594576239585876\n",
      "\n",
      "Confusion matrix:\n",
      "[[693  53]\n",
      " [ 51  36]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  CNN - POS   0.87515   0.404494  0.413793  0.409091\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 72ms/step\n",
      "training data...\n",
      "Accuracy: 0.9532346129417419\n",
      "Loss:0.13985753059387207\n",
      "validation data...\n",
      "Accuracy: 0.7888513803482056\n",
      "Loss:0.5877485275268555\n",
      "\n",
      "Confusion matrix:\n",
      "[[632 114]\n",
      " [ 19  68]]\n",
      "          Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  CNN - Glove & POS  0.840336   0.373626  0.781609  0.505576\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 0s 66ms/step\n",
      "training data...\n",
      "Accuracy: 0.9420109391212463\n",
      "Loss:0.20994392037391663\n",
      "validation data...\n",
      "Accuracy: 0.7466216087341309\n",
      "Loss:0.6449220180511475\n",
      "\n",
      "Confusion matrix:\n",
      "[[708  38]\n",
      " [ 35  52]]\n",
      "     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  C-LSTM - POS  0.912365   0.577778  0.597701  0.587571\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 80ms/step\n",
      "training data...\n",
      "Accuracy: 0.913250207901001\n",
      "Loss:0.2450392246246338\n",
      "validation data...\n",
      "Accuracy: 0.6790540814399719\n",
      "Loss:0.961743950843811\n",
      "\n",
      "Confusion matrix:\n",
      "[[729  17]\n",
      " [ 46  41]]\n",
      "             Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  C-LSTM - GloVe & POS   0.92437   0.706897  0.471264  0.565517\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 1s 129ms/step\n",
      "training data...\n",
      "Accuracy: 0.9431800246238708\n",
      "Loss:0.17040210962295532\n",
      "validation data...\n",
      "Accuracy: 0.775337815284729\n",
      "Loss:0.5994016528129578\n",
      "\n",
      "Confusion matrix:\n",
      "[[709  37]\n",
      " [ 31  56]]\n",
      "                       Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier - POS tags  0.918367   0.602151  0.643678  0.622222\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 126ms/step\n",
      "training data...\n",
      "Accuracy: 0.9516757726669312\n",
      "Loss:0.14284944534301758\n",
      "validation data...\n",
      "Accuracy: 0.7854729890823364\n",
      "Loss:0.48709022998809814\n",
      "\n",
      "Confusion matrix:\n",
      "[[643 103]\n",
      " [ 16  71]]\n",
      "                          Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier - Glove & POS  0.857143   0.408046  0.816092  0.544061\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Dataset B:\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 32ms/step\n",
      "training data...\n",
      "Accuracy: 0.8130163550376892\n",
      "Loss:0.44276162981987\n",
      "validation data...\n",
      "Accuracy: 0.5754950642585754\n",
      "Loss:0.9014223217964172\n",
      "\n",
      "Confusion matrix:\n",
      "[[424  52]\n",
      " [272  76]]\n",
      "   Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM - POS  0.606796    0.59375  0.218391  0.319328\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 79ms/step\n",
      "training data...\n",
      "Accuracy: 0.9413873553276062\n",
      "Loss:0.17599432170391083\n",
      "validation data...\n",
      "Accuracy: 0.6299505233764648\n",
      "Loss:1.0184152126312256\n",
      "\n",
      "Confusion matrix:\n",
      "[[445  31]\n",
      " [231 117]]\n",
      "         Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  LSTM Glove & POS  0.682039   0.790541  0.336207  0.471774\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 33ms/step\n",
      "training data...\n",
      "Accuracy: 0.7993764877319336\n",
      "Loss:0.4685510993003845\n",
      "validation data...\n",
      "Accuracy: 0.5891088843345642\n",
      "Loss:0.7193710207939148\n",
      "\n",
      "Confusion matrix:\n",
      "[[433  43]\n",
      " [256  92]]\n",
      "  Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  CNN - POS  0.637136   0.681481  0.264368  0.380952\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 71ms/step\n",
      "training data...\n",
      "Accuracy: 0.9323460459709167\n",
      "Loss:0.19678238034248352\n",
      "validation data...\n",
      "Accuracy: 0.5668317079544067\n",
      "Loss:1.2493609189987183\n",
      "\n",
      "Confusion matrix:\n",
      "[[457  19]\n",
      " [281  67]]\n",
      "          Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  CNN - Glove & POS  0.635922    0.77907  0.192529  0.308756\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 0s 69ms/step\n",
      "training data...\n",
      "Accuracy: 0.8776305317878723\n",
      "Loss:0.3180478811264038\n",
      "validation data...\n",
      "Accuracy: 0.5804455280303955\n",
      "Loss:0.9837304353713989\n",
      "\n",
      "Confusion matrix:\n",
      "[[456  20]\n",
      " [299  49]]\n",
      "     Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  C-LSTM - POS  0.612864   0.710145  0.140805  0.235012\n",
      "\n",
      "Epoch 00003: early stopping\n",
      "7/7 [==============================] - 0s 71ms/step\n",
      "training data...\n",
      "Accuracy: 0.9533904790878296\n",
      "Loss:0.17257243394851685\n",
      "validation data...\n",
      "Accuracy: 0.603960394859314\n",
      "Loss:1.113967776298523\n",
      "\n",
      "Confusion matrix:\n",
      "[[456  20]\n",
      " [261  87]]\n",
      "             Classifier  Accuracy  Precision  Recall   F-Score\n",
      "0  C-LSTM - GloVe & POS  0.658981   0.813084    0.25  0.382418\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 135ms/step\n",
      "training data...\n",
      "Accuracy: 0.8700701594352722\n",
      "Loss:0.3303679823875427\n",
      "validation data...\n",
      "Accuracy: 0.5742574334144592\n",
      "Loss:0.8150574564933777\n",
      "\n",
      "Confusion matrix:\n",
      "[[457  19]\n",
      " [303  45]]\n",
      "                       Classifier  Accuracy  Precision   Recall   F-Score\n",
      "0  Ensemble Classifier - POS tags  0.609223   0.703125  0.12931  0.218447\n",
      "\n",
      "Epoch 00002: early stopping\n",
      "7/7 [==============================] - 1s 124ms/step\n",
      "training data...\n",
      "Accuracy: 0.918082594871521\n",
      "Loss:0.2309599369764328\n",
      "validation data...\n",
      "Accuracy: 0.6361386179924011\n",
      "Loss:0.7963783144950867\n",
      "\n",
      "Confusion matrix:\n",
      "[[450  26]\n",
      " [243 105]]\n",
      "                          Classifier  Accuracy  Precision    Recall   F-Score\n",
      "0  Ensemble Classifier - Glove & POS  0.673544   0.801527  0.301724  0.438413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to run the experiment\n",
    "# This particular cell has a runtime of around 10-15 min.\n",
    "results7, results8, matrix7, matrix8 = pos_experiment()\n",
    "results7.to_pickle(\"results_7.pkl\")\n",
    "results8.to_pickle(\"results_8.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
